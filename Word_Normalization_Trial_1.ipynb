{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Normalization - Trial 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "pKomgdERz9Tt",
        "colab_type": "code",
        "outputId": "8b3d2c6e-9325-4b86-cc4b-6cda07ebc709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/nasalsabila/kamus-alay/master/colloquial-indonesian-lexicon.csv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-04 00:48:20--  https://raw.githubusercontent.com/nasalsabila/kamus-alay/master/colloquial-indonesian-lexicon.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3093160 (2.9M) [text/plain]\n",
            "Saving to: ‘colloquial-indonesian-lexicon.csv.1’\n",
            "\n",
            "\r          colloquia   0%[                    ]       0  --.-KB/s               \rcolloquial-indonesi 100%[===================>]   2.95M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-01-04 00:48:21 (29.2 MB/s) - ‘colloquial-indonesian-lexicon.csv.1’ saved [3093160/3093160]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HKHa-DJF0Jzc",
        "colab_type": "code",
        "outputId": "0086c26c-3159-409b-b136-cc6b50b464db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1244
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://gist.github.com/f7b7c7758a46da49f84bc68b47997d69.git\n",
        "!bash f7b7c7758a46da49f84bc68b47997d69/pytorch041_cuda92_colab.sh"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'f7b7c7758a46da49f84bc68b47997d69' already exists and is not an empty directory.\n",
            "--2019-01-04 00:48:27--  https://developer.nvidia.com/compute/cuda/9.2/Prod2/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 192.229.189.146\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|192.229.189.146|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/9.2/secure/Prod2/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64.deb?-yARSlRPH2hgt2UnnOmBmw3q0NNp2x86SQ0O5tdy_gDjDfwaCSQPp7Oaw0v76W3hfAJL4qcSRABEe9c1hutpaniQGD1F4u2ZWBAGwNtKbUiFcPftYJg8sTjVKbNGR_7kh8FTQrf0m-kw846fPDZbzcqGg1UYNlQHB6w3pYj3QsBv7N3odpAOr4X5kZfM6768ojhxJBMwlARGCtVB_rftXQ [following]\n",
            "--2019-01-04 00:48:29--  https://developer.download.nvidia.com/compute/cuda/9.2/secure/Prod2/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64.deb?-yARSlRPH2hgt2UnnOmBmw3q0NNp2x86SQ0O5tdy_gDjDfwaCSQPp7Oaw0v76W3hfAJL4qcSRABEe9c1hutpaniQGD1F4u2ZWBAGwNtKbUiFcPftYJg8sTjVKbNGR_7kh8FTQrf0m-kw846fPDZbzcqGg1UYNlQHB6w3pYj3QsBv7N3odpAOr4X5kZfM6768ojhxJBMwlARGCtVB_rftXQ\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 192.229.232.112, 2606:2800:247:2063:46e:21d:825:102e\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|192.229.232.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1267151038 (1.2G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64.1’\n",
            "\n",
            "cuda-repo-ubuntu160 100%[===================>]   1.18G   145MB/s    in 8.0s    \n",
            "\n",
            "2019-01-04 00:48:37 (151 MB/s) - ‘cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64.1’ saved [1267151038/1267151038]\n",
            "\n",
            "\u001b[1;33m\n",
            "WEBGET finished..\n",
            "\u001b[0m\n",
            "(Reading database ... 110915 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64 ...\n",
            "Unpacking cuda-repo-ubuntu1604-9-2-local (9.2.148-1) over (9.2.148-1) ...\n",
            "Setting up cuda-repo-ubuntu1604-9-2-local (9.2.148-1) ...\n",
            "\u001b[1;33m\n",
            "DPKG finished..\n",
            "\u001b[0m\n",
            "OK\n",
            "\u001b[1;33m\n",
            "APT added key..\n",
            "\u001b[0m\n",
            "Get:1 file:/var/cuda-repo-9-2-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-9-2-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-9-2-local  Release [574 B]\n",
            "Get:2 file:/var/cuda-repo-9-2-local  Release [574 B]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64  InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [83.2 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release\n",
            "Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Fetched 247 kB in 2s (115 kB/s)\n",
            "Reading package lists... Done\n",
            "\u001b[1;33m\n",
            "APT update finished..\n",
            "\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cuda is already the newest version (9.2.148-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n",
            "\u001b[1;33m\n",
            "APT finished installing cuda..\n",
            "The CUDA version is: \n",
            "CUDA Version 9.2.148\n",
            "\u001b[0m\n",
            "Requirement already satisfied: torch==0.4.1 from http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "\u001b[1;33m\n",
            "pip fininshed installing PyTorch 0.4.1 with CUDA 9.2 backend..\n",
            "\u001b[0m\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n",
            "\u001b[1;33m\n",
            "pip finished installing torchvision..\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1lwAsbTy004U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CugViWUXjj0V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xZ-Vm8mW08kD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch import nn, optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qc9bbAV32Er4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\n",
        "    'cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IrkycdZ_2LjJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('colloquial-indonesian-lexicon.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g9vemmss2R6g",
        "colab_type": "code",
        "outputId": "7570aeb6-ade8-45dd-a329-a7071d2bd113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>slang</th>\n",
              "      <th>formal</th>\n",
              "      <th>In-dictionary</th>\n",
              "      <th>context</th>\n",
              "      <th>category1</th>\n",
              "      <th>category2</th>\n",
              "      <th>category3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>woww</td>\n",
              "      <td>wow</td>\n",
              "      <td>1</td>\n",
              "      <td>wow</td>\n",
              "      <td>elongasi</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aminn</td>\n",
              "      <td>amin</td>\n",
              "      <td>1</td>\n",
              "      <td>Selamat ulang tahun kakak tulus semoga panjang...</td>\n",
              "      <td>elongasi</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>met</td>\n",
              "      <td>selamat</td>\n",
              "      <td>1</td>\n",
              "      <td>Met hari netaas kak!? Wish you all the best @t...</td>\n",
              "      <td>abreviasi</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>netaas</td>\n",
              "      <td>menetas</td>\n",
              "      <td>1</td>\n",
              "      <td>Met hari netaas kak!? Wish you all the best @t...</td>\n",
              "      <td>afiksasi</td>\n",
              "      <td>elongasi</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>keberpa</td>\n",
              "      <td>keberapa</td>\n",
              "      <td>0</td>\n",
              "      <td>Birthday yg keberpa kak?</td>\n",
              "      <td>abreviasi</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     slang    formal  In-dictionary  \\\n",
              "0     woww       wow              1   \n",
              "1    aminn      amin              1   \n",
              "2      met   selamat              1   \n",
              "3   netaas   menetas              1   \n",
              "4  keberpa  keberapa              0   \n",
              "\n",
              "                                             context  category1 category2  \\\n",
              "0                                                wow   elongasi         0   \n",
              "1  Selamat ulang tahun kakak tulus semoga panjang...   elongasi         0   \n",
              "2  Met hari netaas kak!? Wish you all the best @t...  abreviasi         0   \n",
              "3  Met hari netaas kak!? Wish you all the best @t...   afiksasi  elongasi   \n",
              "4                           Birthday yg keberpa kak?  abreviasi         0   \n",
              "\n",
              "  category3  \n",
              "0         0  \n",
              "1         0  \n",
              "2         0  \n",
              "3         0  \n",
              "4         0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "ruTL2ZUM2cKu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "all66dsJ2co2",
        "colab_type": "code",
        "outputId": "bfbda9a5-8c0c-40a3-c6e9-6eb28c2cbc6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "cell_type": "code",
      "source": [
        "df.groupby('category1').agg('count')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>slang</th>\n",
              "      <th>formal</th>\n",
              "      <th>In-dictionary</th>\n",
              "      <th>context</th>\n",
              "      <th>category2</th>\n",
              "      <th>category3</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category1</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>abreviasi</th>\n",
              "      <td>7162</td>\n",
              "      <td>7162</td>\n",
              "      <td>7162</td>\n",
              "      <td>7162</td>\n",
              "      <td>7162</td>\n",
              "      <td>7162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>afiksasi</th>\n",
              "      <td>1089</td>\n",
              "      <td>1089</td>\n",
              "      <td>1089</td>\n",
              "      <td>1089</td>\n",
              "      <td>1089</td>\n",
              "      <td>1089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>akronim</th>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "      <td>256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anaptiksis</th>\n",
              "      <td>404</td>\n",
              "      <td>404</td>\n",
              "      <td>404</td>\n",
              "      <td>404</td>\n",
              "      <td>404</td>\n",
              "      <td>404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>coinage</th>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>elongasi</th>\n",
              "      <td>1906</td>\n",
              "      <td>1906</td>\n",
              "      <td>1906</td>\n",
              "      <td>1906</td>\n",
              "      <td>1906</td>\n",
              "      <td>1906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>homofon</th>\n",
              "      <td>169</td>\n",
              "      <td>169</td>\n",
              "      <td>169</td>\n",
              "      <td>169</td>\n",
              "      <td>169</td>\n",
              "      <td>169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>metatesis</th>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>modifikasi vokal</th>\n",
              "      <td>626</td>\n",
              "      <td>626</td>\n",
              "      <td>626</td>\n",
              "      <td>626</td>\n",
              "      <td>626</td>\n",
              "      <td>626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>monoftongisasi</th>\n",
              "      <td>208</td>\n",
              "      <td>208</td>\n",
              "      <td>208</td>\n",
              "      <td>208</td>\n",
              "      <td>208</td>\n",
              "      <td>208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>naturalisasi</th>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pungtuasi</th>\n",
              "      <td>42</td>\n",
              "      <td>42</td>\n",
              "      <td>42</td>\n",
              "      <td>42</td>\n",
              "      <td>42</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>reduplikasi</th>\n",
              "      <td>546</td>\n",
              "      <td>546</td>\n",
              "      <td>546</td>\n",
              "      <td>546</td>\n",
              "      <td>546</td>\n",
              "      <td>546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>salah ketik</th>\n",
              "      <td>120</td>\n",
              "      <td>120</td>\n",
              "      <td>120</td>\n",
              "      <td>120</td>\n",
              "      <td>120</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>subtitusi</th>\n",
              "      <td>216</td>\n",
              "      <td>216</td>\n",
              "      <td>216</td>\n",
              "      <td>216</td>\n",
              "      <td>216</td>\n",
              "      <td>216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word-value letter</th>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "      <td>128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zeroisasi</th>\n",
              "      <td>2063</td>\n",
              "      <td>2063</td>\n",
              "      <td>2063</td>\n",
              "      <td>2063</td>\n",
              "      <td>2063</td>\n",
              "      <td>2063</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   slang  formal  In-dictionary  context  category2  category3\n",
              "category1                                                                     \n",
              "abreviasi           7162    7162           7162     7162       7162       7162\n",
              "afiksasi            1089    1089           1089     1089       1089       1089\n",
              "akronim              256     256            256      256        256        256\n",
              "anaptiksis           404     404            404      404        404        404\n",
              "coinage               50      50             50       50         50         50\n",
              "elongasi            1906    1906           1906     1906       1906       1906\n",
              "homofon              169     169            169      169        169        169\n",
              "metatesis             13      13             13       13         13         13\n",
              "modifikasi vokal     626     626            626      626        626        626\n",
              "monoftongisasi       208     208            208      208        208        208\n",
              "naturalisasi           8       8              8        8          8          8\n",
              "pungtuasi             42      42             42       42         42         42\n",
              "reduplikasi          546     546            546      546        546        546\n",
              "salah ketik          120     120            120      120        120        120\n",
              "subtitusi            216     216            216      216        216        216\n",
              "word-value letter    128     128            128      128        128        128\n",
              "zeroisasi           2063    2063           2063     2063       2063       2063"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "QXyphd_9kXT2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = df['slang']\n",
        "y = df['formal']\n",
        "SOS_TOKEN = 0\n",
        "PAD_TOKEN = ord('*')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tdjjpohq2jsG",
        "colab_type": "code",
        "outputId": "886d6ae2-0516-47b2-9753-c56d89668e98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def apply_padding(_str, n_max_char, zero_char='*'):\n",
        "  len_str = len(_str)\n",
        "  if len_str >= n_max_char:\n",
        "    return _str[:n_max_char]\n",
        "  \n",
        "  else:\n",
        "    return _str + ''.join([zero_char for t in range(n_max_char - len_str)])\n",
        "  \n",
        "print(apply_padding('abcd', 10))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "abcd******\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vPptadJqmnrw",
        "colab_type": "code",
        "outputId": "9bd37f36-67bb-4f9c-c383-275d51b761f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "max_len_slang = X.apply(len).max()\n",
        "max_len_formal = y.apply(len).max()\n",
        "print('max_len_slang: {}, max_len_formal {}'.format(max_len_slang, max_len_formal))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_len_slang: 59, max_len_formal 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vOGnsGMiq2CM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lord(x):\n",
        "  \"\"\"make string into integer\"\"\"\n",
        "  return [SOS_TOKEN] + [ord(x1) for x1 in x]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IyA8PzaHnOu6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = X.apply(apply_padding, args=(max_len_slang,)).apply(list).apply(np.array).apply(lord)\n",
        "y = y.apply(apply_padding, args=(max_len_slang,)).apply(list).apply(np.array).apply(lord)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VC_58BfJnz7q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7AsLMbvHFfvr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_torch(x, device=device):\n",
        "  return torch.from_numpy(np.array(x.values.tolist())).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2rMMkCGvoL6t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, hidden_size, vocab_size, embed_dim):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "    self.gru = nn.GRU(vocab_size * embed_dim, hidden_size)\n",
        "    \n",
        "    self.hidden_size = hidden_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_dim = embed_dim\n",
        "    \n",
        "  def forward(self, x, hidden):\n",
        "    embed = self.embedding(x).view(1, 1, -1)\n",
        "    output, hidden = self.gru(embed, hidden)\n",
        "    return output, hidden\n",
        "  \n",
        "  def init_hidden(self, device):\n",
        "    return torch.zeros(1,1,self.hidden_size, device=device)\n",
        "  \n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, hidden_size, vocab_size, embed_dim, out_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "    self.gru = nn.GRU(vocab_size * embed_dim, hidden_size)\n",
        "    self.l1 = nn.Linear(hidden_size, out_size)\n",
        "    self.sm = nn.LogSoftmax(dim=1)\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "  def forward(self, x, hidden):\n",
        "    out = self.embedding(x).view(1,1,-1)\n",
        "    out = F.relu(out)\n",
        "    out, out_hidden = self.gru(out, hidden)\n",
        "    out = self.sm(self.l1(out[0]))\n",
        "    \n",
        "    return out, out_hidden\n",
        "  \n",
        "  def init_hidden(self, device):\n",
        "    return torch.zeros(1,1,self.hidden_size, device=device)\n",
        "  \n",
        "class SimpleSeq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, vocab_size):\n",
        "    super(SimpleSeq2Seq, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.vocab_size = vocab_size\n",
        "    \n",
        "  def forward(self, x, y, device=device):\n",
        "    max_len_translation = y.shape[0] \n",
        "    encoder_hidden = self.encoder.init_hidden(device)\n",
        "    encoded, encoder_hidden = self.encoder(x, encoder_hidden)\n",
        "    \n",
        "    # buffer for full outputs\n",
        "    outputs = list()\n",
        "    # make tensor to have dimension\n",
        "    # or in other words make into list (in tensor)\n",
        "    _input = y[:1]\n",
        "    # skip the first token since it will be SOS\n",
        "    for t in range(1, max_len_translation):\n",
        "      output, decoder_hidden = self.decoder(x, encoder_hidden)\n",
        "      outputs.append(output.view(-1))\n",
        "      _input = output\n",
        "      \n",
        "    return torch.cat(outputs).view(max_len_translation - 1, -1).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QypFiU1Wwwz3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# hidden_size = \n",
        "input_size = 60\n",
        "vocab_size = 122\n",
        "embed_dim = 60\n",
        "hidden_dim = 256\n",
        "\n",
        "encoder = Encoder(hidden_dim, vocab_size, embed_dim).to(device)\n",
        "decoder = Decoder(hidden_dim, vocab_size, embed_dim, vocab_size).to(device)\n",
        "seq = SimpleSeq2Seq(encoder, decoder, vocab_size).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "\n",
        "optimizer = optim.Adam(seq.parameters())\n",
        "\n",
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TyEVYqqo_omU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Xt_torch, yt_torch = to_torch(X_train), to_torch(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EGFps79368Cv",
        "colab_type": "code",
        "outputId": "96ac6a90-7fc1-466d-ce0f-42c3e46311fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "cell_type": "code",
      "source": [
        "epoch_loss = 0\n",
        "max_len_translation = input_size\n",
        "seq.train()\n",
        "\n",
        "for e in range(epochs):\n",
        "  len_iter = 0\n",
        "  for src, trg in zip(Xt_torch, yt_torch):\n",
        "    len_iter += 1\n",
        "    src = src.to(device)\n",
        "    trg = trg.to(device)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = seq(src, trg)\n",
        "    \n",
        "    loss = criterion(output, trg[1:])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "  \n",
        "  print('loss: {}'.format(epoch_loss/len(len_iter)))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-baeee8a006a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:26"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "bVt_cat5rgci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_hidden = encoder.init_hidden(device)\n",
        "decoder_hidden = decoder.init_hidden(device)\n",
        "running_loss = 0\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "epochs = 20\n",
        "loss = 0\n",
        "\n",
        "for e in range(epochs):\n",
        "  for slang, target in zip(to_torch(X_train), to_torch(y_train)):\n",
        "    slang = slang.to(device)\n",
        "    target = target.to(device)\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    encoded, encoder_hidden = encoder(slang, encoder_hidden)\n",
        "    decoded, decoder_hidden = decoder(target, encoder_hidden)\n",
        "    \n",
        "    \n",
        "    target_len = target.shape[0]\n",
        "    for i in range(target_len):\n",
        "      \n",
        "      loss += criterion(decoded[0][i].view(-1), target[i].view(-1))\n",
        "      loss.backward()\n",
        "      encoder_optimizer.step()\n",
        "      decoder_optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    \n",
        "    \n",
        "  else:\n",
        "    test_loss = 0 \n",
        "    with torch.no_grad():\n",
        "      for slang, target in zip(to_torch(X_test), to_torch(y_test)):\n",
        "        slang = slang.to(device)\n",
        "        target = target.to(device)\n",
        "        encoded, encoder_hidden = encoder(slang, encoder_hidden)\n",
        "        decoded, decoder_hidden = decoder(target, encoder_hidden)\n",
        "        target_len = target.shape[0]\n",
        "        for i in range(target_len):\n",
        "          loss = criterion(decoded[0][i].view(-1), target[i].view(-1))\n",
        "          test_loss += loss.item()\n",
        "        \n",
        "        res = decoded.view(*target.shape)\n",
        "        equals = res == target\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "        \n",
        "    print('epoch: {} ---------------'.format(e))\n",
        "    train_losses.append(running_loss/X_train.shape[0])\n",
        "    test_losses.append(test_loss/X_test.shape[0])\n",
        "    \n",
        "    \n",
        "  \n",
        "        \n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_EtzqQXJR9x3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tembed.embedding(to_torch(X_train[:1])[0]).view(1,-1).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ipd68eprUaxY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
        "# we should tokenize the input, but we will ignore that for now\n",
        "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
        "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
        "            for i in range(len(test_sentence) - 2)]\n",
        "# print the first 3, just so you can see what they look like\n",
        "print(trigrams[:3])\n",
        "\n",
        "vocab = set(test_sentence)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GNLSYNn-KeC6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "e = nn.Embedding(len(vocab), 10)\n",
        "l = nn.Linear(20, 128)\n",
        "context, target = trigrams[0]\n",
        "context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "print(context_idxs.shape)\n",
        "_e = e(context_idxs).view((1, -1))\n",
        "print(_e.shape)\n",
        "_e = l(_e)\n",
        "print(_e.shape)\n",
        "_e\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pE_pHBB9W3Xe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D3hhwpTeVDYO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trigrams[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8_D4F13rHe3Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rmmiphPWEdTB",
        "colab_type": "code",
        "outputId": "d56ec958-7e00-4ea9-81f2-04006d2f7d96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.max(\n",
        "    np.unique(np.array(X.values.tolist()).ravel())\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "122"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    }
  ]
}